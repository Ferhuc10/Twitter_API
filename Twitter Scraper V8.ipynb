{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.9/site-packages (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.21.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.27.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (61.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.9.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.3)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fernandohuerta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fernandohuerta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitterscraper in /opt/anaconda3/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.9/site-packages (from twitterscraper) (4.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from twitterscraper) (2.27.1)\n",
      "Requirement already satisfied: billiard in /opt/anaconda3/lib/python3.9/site-packages (from twitterscraper) (4.0.0)\n",
      "Requirement already satisfied: coala-utils~=0.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from twitterscraper) (0.5.1)\n",
      "Requirement already satisfied: bs4 in /opt/anaconda3/lib/python3.9/site-packages (from twitterscraper) (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (from bs4->twitterscraper) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->bs4->twitterscraper) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->twitterscraper) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->twitterscraper) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->twitterscraper) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests->twitterscraper) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtwitterscraper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m query_tweets\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtwitterscraper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m query_tweets_from_user\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdt\u001b[39;00m \n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/twitterscraper/__init__.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAhmet Taspinar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m __license__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMIT\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtwitterscraper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m query_tweets\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtwitterscraper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m query_tweets_from_user\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtwitterscraper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m query_user_info\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/twitterscraper/query.py:76\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m start \u001b[38;5;241m+\u001b[39m h \u001b[38;5;241m*\u001b[39m i\n\u001b[0;32m---> 76\u001b[0m proxies \u001b[38;5;241m=\u001b[39m \u001b[43mget_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m proxy_pool \u001b[38;5;241m=\u001b[39m cycle(proxies)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_single_page\u001b[39m(query, lang, pos, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, from_user\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, use_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/twitterscraper/query.py:49\u001b[0m, in \u001b[0;36mget_proxies\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproxylisttable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m list_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m list_td \u001b[38;5;241m=\u001b[39m [elem\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m list_tr]\n\u001b[1;32m     51\u001b[0m list_td \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, list_td))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "from twitterscraper.query import query_tweets_from_user\n",
    "import datetime as dt \n",
    "import pandas as pd \n",
    "#import twitterscraper as tw\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r=requests.get(\"https://twitter.com\")\n",
    "c=r.content\n",
    "\n",
    "soup=BeautifulSoup(c,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "#begin_date = dt.date(2015,1,1)\n",
    "#end_date = dt.date(2022,7,14)\n",
    "\n",
    "#limit = 1000\n",
    "#lang = 'english'\n",
    "\n",
    "#Use this to search a specific user\n",
    "\n",
    "#user = 'MVROM'\n",
    "\n",
    "#tweets = query_tweets_from_user(user)\n",
    "\n",
    "#df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "\n",
    "#df = df.loc[df['screen_name'] == user]\n",
    "\n",
    "#df = df['text']\n",
    "\n",
    "#df\n",
    "\n",
    "#Use this if wanting to seach for a specific Phrase or word\n",
    "\n",
    "#tweets = query_tweets('impeachment', begindate = begin_date, enddate = end_date, limit = limit, lang = lang)\n",
    "#df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "\n",
    "#df = df['text']\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This splits all the sentences up which makes it easier for us to work with\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "for word in df:\n",
    "    all_sentences.append(word)\n",
    "\n",
    "all_sentences\n",
    "#df1 = df.to_string()\n",
    "\n",
    "#df_split = df1.split()\n",
    "\n",
    "#df_split\n",
    "lines = list()\n",
    "for line in all_sentences:    \n",
    "    words = line.split()\n",
    "    for w in words: \n",
    "       lines.append(w)\n",
    "\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "\n",
    "lines = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in lines]\n",
    "\n",
    "lines\n",
    "\n",
    "lines2 = []\n",
    "\n",
    "for word in lines:\n",
    "    if word != '':\n",
    "        lines2.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is stemming the words to their root\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stem = []\n",
    "for word in lines2:\n",
    "    stem.append(s_stemmer.stem(word))\n",
    "    \n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all Stop Words\n",
    "\n",
    "stem2 = []\n",
    "\n",
    "for word in stem:\n",
    "    if word not in nlp.Defaults.stop_words:\n",
    "        stem2.append(word)\n",
    "\n",
    "stem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(stem2)\n",
    "\n",
    "df = df[0].value_counts()\n",
    "\n",
    "#df\n",
    "#df['freq'] = df.groupby(0)[0].transform('count')\n",
    "#df['freq'] = df.groupby(0)[0].transform('count')\n",
    "#df.sort_values(by = ('freq'), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will give frequencies of our words\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "freqdoctor = FreqDist()\n",
    "\n",
    "for words in df:\n",
    "    freqdoctor[words] += 1\n",
    "\n",
    "freqdoctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a simple plot that shows the top 20 words being used\n",
    "#df.plot(20)\n",
    "\n",
    "df = df[:20,]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(df.values, df.index, alpha=0.8)\n",
    "plt.title('Top Words Overall')\n",
    "plt.ylabel('Word from Tweet', fontsize=12)\n",
    "plt.xlabel('Count of Words', fontsize=12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \" \" \n",
    "stem2 = str1.join(lines2)\n",
    "\n",
    "stem2 = nlp(stem2)\n",
    "\n",
    "label = [(X.text, X.label_) for X in stem2.ents]\n",
    "\n",
    "df6 = pd.DataFrame(label, columns = ['Word','Entity'])\n",
    "\n",
    "df7 = df6.where(df6['Entity'] == 'ORG')\n",
    "\n",
    "df7 = df7['Word'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df7[:20,]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(df.values, df.index, alpha=0.8)\n",
    "plt.title('Top Organizations Mentioned')\n",
    "plt.ylabel('Word from Tweet', fontsize=12)\n",
    "plt.xlabel('Count of Words', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \" \" \n",
    "stem2 = str1.join(lines2)\n",
    "\n",
    "stem2 = nlp(stem2)\n",
    "\n",
    "label = [(X.text, X.label_) for X in stem2.ents]\n",
    "\n",
    "df10 = pd.DataFrame(label, columns = ['Word','Entity'])\n",
    "\n",
    "df10 = df10.where(df10['Entity'] == 'PERSON')\n",
    "\n",
    "df11 = df10['Word'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df11[:20,]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(df.values, df.index, alpha=0.8)\n",
    "plt.title('Top People Mentioned')\n",
    "plt.ylabel('Word from Tweet', fontsize=12)\n",
    "plt.xlabel('Count of Words', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
